<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on The Think Tank</title>
        <link>https://anoura12.github.io/blog/post/</link>
        <description>Recent content in Posts on The Think Tank</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sat, 07 Oct 2023 14:45:50 +0530</lastBuildDate><atom:link href="https://anoura12.github.io/blog/post/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Evaluating LLMs Part I - Benchmarking Strategies</title>
        <link>https://anoura12.github.io/blog/p/evaluating-llms-part-i-benchmarking-strategies/</link>
        <pubDate>Sat, 07 Oct 2023 14:45:50 +0530</pubDate>
        
        <guid>https://anoura12.github.io/blog/p/evaluating-llms-part-i-benchmarking-strategies/</guid>
        <description>&lt;h1 id=&#34;why-do-we-need-to-evaluate-llms&#34;&gt;Why do we need to evaluate LLMs?&lt;/h1&gt;
&lt;p&gt;It is a well known fact in the LLM world that evaluating large language models is no easy task. It&amp;rsquo;s something that researchers are still trying to perfect even today. For the end user specifically, creating LLM applications that deliver value is of utmost importance and as developers we need to understand how to measure that value. Josh Tobin, an OpenAI researcher, explains that with the advent of generative AI, anyone can create an MVP within 15 minutes. From then on, a lot of people underestimate the amount of effort required to convert that MVP into a full fledged product. Robust evaluation processes are needed to create reliable production ready LLM systems that are much more than just a simple rudimentary prototype.&lt;/p&gt;
&lt;h1 id=&#34;why-is-it-so-hard-to-evaluate-llms&#34;&gt;Why is it so hard to evaluate LLMs?&lt;/h1&gt;
&lt;p&gt;The main issue with evaluating LLM outputs or any kind of generative AI output is that there really isn&amp;rsquo;t any kind of &amp;ldquo;right&amp;rdquo; or &amp;ldquo;wrong&amp;rdquo; answer. Take for example, describing what a Golden Retriever looks like. Objectively yes, there is a baseline requirement for a dog&amp;rsquo;s basic features to be there but an LLM can output two different descriptions of a Golden Retriever which can be accurate. The question here is how would we quantify which of the two descriptions is better? Moreover, with a diverse set of outputs, some cases might be applicable&lt;/p&gt;
&lt;p&gt;So&amp;hellip; what do we do?&lt;/p&gt;
&lt;h1 id=&#34;introducing-benchmarking&#34;&gt;Introducing Benchmarking!&lt;/h1&gt;
&lt;p&gt;Since the beginning of time, we&amp;rsquo;ve evaluated human performance through standardised tests like the SATs or GREs so why can&amp;rsquo;t we employ that for evaluating LLMs? Benchmarking does just that.&lt;/p&gt;
&lt;p&gt;Benchmarking simply refers to standardised software performance tests.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Write like Agatha Christie using NanoGPT!</title>
        <link>https://anoura12.github.io/blog/p/write-like-agatha-christie-using-nanogpt/</link>
        <pubDate>Wed, 04 Oct 2023 14:24:06 +0530</pubDate>
        
        <guid>https://anoura12.github.io/blog/p/write-like-agatha-christie-using-nanogpt/</guid>
        <description>&lt;img src="https://anoura12.github.io/blog/p/write-like-agatha-christie-using-nanogpt/agathabooks.jpg" alt="Featured image of post Write like Agatha Christie using NanoGPT!" /&gt;&lt;p&gt;Employing LLMs to do what we want is now possible through prompting and finetuning. For specific downstream tasks, we can finetune the LLM on our own custom dataset to generate the outputs that we want. However, finetuning is computationally expensive requiring GPUs to train these LLMs so most of the times, we can&amp;rsquo;t fientune LLMs using just our computer.&lt;/p&gt;
&lt;p&gt;Andrej Karpathy from OpenAI created a simple framework called nanoGPT that enables you to train and finetune medium sized GPTs. It gives you an interesting insight into how a GPT works and allows you to use your custom dataset to train these models. His &lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=kCc8FmEb1nY&amp;amp;t=2265s&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;video&lt;/a&gt; on building a GPT from scratch, breaks down the concepts of a GPT&amp;rsquo;s architecture and what really goes on under the hood.&lt;/p&gt;
&lt;p&gt;Being an avid fan of Agatha Christie&amp;rsquo;s Hercule Poirot mystery series, I decided to train and finetune a GPT to generating text that somewhat resembles her writing and enjoy reading a never seen before mystery story in the hand of Agatha Christie!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/w4Jbv7uv4n1ug/giphy.gif&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;training&#34;&gt;Training&lt;/h1&gt;
&lt;p&gt;There are three stages to producing a piece of text through training a GPT on your dataset in nanoGPT&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prepare&lt;/li&gt;
&lt;li&gt;Train&lt;/li&gt;
&lt;li&gt;Sample&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;prepare&#34;&gt;Prepare&lt;/h2&gt;
&lt;p&gt;This stage involves loading your dataset (in this case, a text file) and creating tokens through the OpenAI BPE tokeniser called tiktoken. For the dataset, I used Agatha Christie&amp;rsquo;s Poirot Investigates book which is a series of 8 short murder mystery stories.&lt;/p&gt;
&lt;p&gt;I initially thought of getting one of her novels which contained one mystery and inputting that into the model directly for training however, that didn&amp;rsquo;t work out so well. Choosing a list of short stories which contains a mystery from start to end would give the model several examples of her writing in a short dataset which would be more feasible than working with several novels for our experiment.&lt;/p&gt;
&lt;p&gt;For every new dataset, we need to create a prepare.py file which ultimately divides the tokens into training(train.bin) and validation (val.bin) data. I stored the file under a folder called &lt;code&gt;agatha&lt;/code&gt; in the &lt;code&gt;data&lt;/code&gt; folder.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import os
import requests
import tiktoken
import numpy as np

input_file_path = os.path.join(os.path.dirname(_file_), &#39;poirot.txt&#39;)
with open(input_file_path, &#39;r&#39;) as f:
    data = f.read()
n = len(data)
train_data = data[:int(n*0.9)]
val_data = data[int(n*0.9):]

# encode with tiktoken gpt2 bpe
enc = tiktoken.get_encoding(&amp;quot;gpt2&amp;quot;)
train_ids = enc.encode_ordinary(train_data)
val_ids = enc.encode_ordinary(val_data)
print(f&amp;quot;train has {len(train_ids):,} tokens&amp;quot;)
print(f&amp;quot;val has {len(val_ids):,} tokens&amp;quot;)

# export to bin files
train_ids = np.array(train_ids, dtype=np.uint16)
val_ids = np.array(val_ids, dtype=np.uint16)
train_ids.tofile(os.path.join(os.path.dirname(_file_), &#39;train.bin&#39;))
val_ids.tofile(os.path.join(os.path.dirname(_file_), &#39;val.bin&#39;))
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To run the prepare.py file, run this command.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python data/agatha/prepare.py
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You should get something like this after running the command&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://anoura12.github.io/blog/blog/p/write-like-agatha-christie-using-nanogpt/train_val.jpeg&#34;
	width=&#34;1030&#34;
	height=&#34;77&#34;
	srcset=&#34;https://anoura12.github.io/blog/blog/p/write-like-agatha-christie-using-nanogpt/train_val_hub2e41e221a363b43999e7043641b983a_20969_480x0_resize_q75_box.jpeg 480w, https://anoura12.github.io/blog/blog/p/write-like-agatha-christie-using-nanogpt/train_val_hub2e41e221a363b43999e7043641b983a_20969_1024x0_resize_q75_box.jpeg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;tokens&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;1337&#34;
		data-flex-basis=&#34;3210px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;train&#34;&gt;Train&lt;/h2&gt;
&lt;p&gt;Next, we train the model on the training data by running this command.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python train.py config/train_poirot.py
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;So, as you can see there are two files in question here - the train.py and train_poirot.py files.&lt;/p&gt;
&lt;p&gt;The train.py file defines a set of parameters and the code dependent on the kind of device (cpu, cuda, mps etc.) you&amp;rsquo;re using. For this experiment, I used a single NVIDIA GPU to run around 5000 iterations for training the model. The training process involves the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Fetch the first batch of data (batch_size = 32)&lt;/li&gt;
&lt;li&gt;Set the learning rate&lt;/li&gt;
&lt;li&gt;Evaluate loss&lt;/li&gt;
&lt;li&gt;Capture checkpoints which is done everytime the loss decreases&lt;/li&gt;
&lt;li&gt;Perform a forward backward pass (update the model)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The train_poirot.py file is a neat way to store all the parameters that are set to your custom values that override the default values in train.py. Here&amp;rsquo;s what the main parameters mean:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;gradient_accumulation_steps&lt;/code&gt; - This is the number of batches after which a gradient update should happen.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;batch_size&lt;/code&gt; - This refers to the number of data samples processed in one forward pass&lt;/li&gt;
&lt;li&gt;&lt;code&gt;block_size&lt;/code&gt; - This refers to the context length. The longer the context length, the more computationally expensive it will get.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;n_layer&lt;/code&gt; - The number of layers in the model, more the layers more complex is your model&lt;/li&gt;
&lt;li&gt;&lt;code&gt;n_head&lt;/code&gt; - The number of attention heads in the multi-layer attention part of the transformer&lt;/li&gt;
&lt;li&gt;&lt;code&gt;n_embd&lt;/code&gt; - The dimensionality of embedding vectors, higher the value, higher the complexity&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dropout&lt;/code&gt; - Helps prevent overfitting by setting a number of inputs to zero. So, a dropout value of 0.2 refers to 20% of the input values being zero.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;learning rate&lt;/code&gt; - Step-size at which model&amp;rsquo;s parameters are updated&lt;/li&gt;
&lt;li&gt;&lt;code&gt;max_iters&lt;/code&gt; - Maximum number of iterations which the model is trained on&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;out_dir = &#39;out-poirot&#39;
eval_interval = 250 # keep frequent because we&#39;ll overfit
eval_iters = 200
log_interval = 10 # don&#39;t print too too often

# we expect to overfit on this small dataset, so only save when val improves
always_save_checkpoint = False

wandb_log = False # override via command line if you like
wandb_project = &#39;agatha&#39;
wandb_run_name = &#39;mini-gpt&#39;

dataset = &#39;agatha&#39;
gradient_accumulation_steps = 1
batch_size = 32
block_size = 256 # context of up to 256 previous characters

# baby GPT model :)
n_layer = 6
n_head = 6
n_embd = 384
dropout = 0.2

learning_rate = 1e-3 # with baby networks can afford to go a bit higher
max_iters = 5000
lr_decay_iters = 5000 # make equal to max_iters usually
min_lr = 1e-4 # learning_rate / 10 usually
beta2 = 0.99 # make a bit bigger because number of tokens per iter is small

warmup_iters = 100 # not super necessary potentially
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This is how the training process looks like. &lt;code&gt;mfu&lt;/code&gt; stands for Memory Footprint Utilisation which indicates the amount of memory used for each iteration of the training process as a percentage.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://anoura12.github.io/blog/blog/p/write-like-agatha-christie-using-nanogpt/train.jpeg&#34;
	width=&#34;900&#34;
	height=&#34;559&#34;
	srcset=&#34;https://anoura12.github.io/blog/blog/p/write-like-agatha-christie-using-nanogpt/train_hu4a227be7b74c3f19b47b34d1fb0f79e6_148701_480x0_resize_q75_box.jpeg 480w, https://anoura12.github.io/blog/blog/p/write-like-agatha-christie-using-nanogpt/train_hu4a227be7b74c3f19b47b34d1fb0f79e6_148701_1024x0_resize_q75_box.jpeg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;training&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;161&#34;
		data-flex-basis=&#34;386px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;sample&#34;&gt;Sample&lt;/h2&gt;
&lt;p&gt;This stage involves generating the text after the model has been trained. The file also conatins majority of the code defined in train.py except for the following piece of code. It refers to the checkpoint contained in &lt;code&gt;out-poirot&lt;/code&gt; to generate text.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# encode the beginning of the prompt
if start.startswith(&#39;FILE:&#39;):
    with open(start[5:], &#39;r&#39;, encoding=&#39;utf-8&#39;) as f:
        start = f.read()
start_ids = encode(start)
x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])

# run generation
with torch.no_grad():
    with ctx:
        for k in range(num_samples):
            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)
            print(decode(y[0].tolist()))
            print(&#39;---------------&#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To generate text, run this command.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python sample.py --out_dir=out-poirot
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Annddd this is the output&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&#39;And that I&#39;d got to have been
belant is that I should be really to hurry over, Hastings, but he was no. I am
have the mysterious man, my friend.&#39;

The little bottle to take a lady&#39;s nurse, and the stairs. He is no further supply into the disaster. In a

&#39;I didn&#39;t it wasps&#39;But I ami. It&#39;s death, eh, I was likely to scorn, Lady Juliet
your mind. She was a little husband&#39;s a moment! But they feel
partly and
not yet my little affair on the garden, you reply. She, but I was a sharp face waspsâ€” and her husband, the window.
evening, however, madame, you know that she
friend, Monsieurier&#39;s eight, but no time, there was
on one, I knew that she slipped to be tired. He is the paper for your time, though she had
never just for you say, the smoking-five and a moustache. I knew
window, did not be the boy to see her, and left the library.

to see, Poirot. He
But it somewhere.&#39;
&#39;I know this evening.&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;It doesn&amp;rsquo;t make a lot of sense but the words themselves sort of resemble her writing?&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s try to do better with finetuning.&lt;/p&gt;
&lt;h1 id=&#34;finetuning&#34;&gt;Finetuning&lt;/h1&gt;
&lt;p&gt;The process for finetuning remains the same except this time we refer to a different set of parameters defined in &lt;code&gt;finetune_poirot.py&lt;/code&gt;. In this file, we load an existing pretrained model (gpt2-large - 776 million params!)
and finetune that to our usecase.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import time

out_dir = &#39;out-poirot&#39;
eval_interval = 5
eval_iters = 40
wandb_log = False # feel free to turn on
wandb_project = &#39;agatha&#39;
wandb_run_name = &#39;ft-&#39; + str(time.time())

dataset = &#39;agatha&#39;
init_from = &#39;gpt2-large&#39; # this is the largest GPT-2 model

# only save checkpoints if the validation loss improves
always_save_checkpoint = False

batch_size = 1
gradient_accumulation_steps = 32
max_iters = 10

# finetune at constant LR
learning_rate = 3e-5
decay_lr = False
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To run this file, we use the following command.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python train.py config/finetune_poirot.py
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This took a lot longer than expected and I managed to get 10 iterations out of it&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://anoura12.github.io/blog/blog/p/write-like-agatha-christie-using-nanogpt/finetune.jpeg&#34;
	width=&#34;1288&#34;
	height=&#34;482&#34;
	srcset=&#34;https://anoura12.github.io/blog/blog/p/write-like-agatha-christie-using-nanogpt/finetune_hu633a271990deba70090d9b59fca5a233_125742_480x0_resize_q75_box.jpeg 480w, https://anoura12.github.io/blog/blog/p/write-like-agatha-christie-using-nanogpt/finetune_hu633a271990deba70090d9b59fca5a233_125742_1024x0_resize_q75_box.jpeg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Finetuning&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;267&#34;
		data-flex-basis=&#34;641px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Next, we define a prompt.txt file which is converted to embeddings in the sample.py file. Initially, I used the following prompt - In the style of Agatha Christie, craft a short story where an unexpected guest at a remote country estate dinner party ends up murdered, leaving the guests to unravel the secrets and motives hidden among them.&lt;/p&gt;
&lt;p&gt;It got me some pretty silly results&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://anoura12.github.io/blog/blog/p/write-like-agatha-christie-using-nanogpt/silly.jpeg&#34;
	width=&#34;1600&#34;
	height=&#34;366&#34;
	srcset=&#34;https://anoura12.github.io/blog/blog/p/write-like-agatha-christie-using-nanogpt/silly_hu941e12d02283ce2e345f7b36857c634a_77313_480x0_resize_q75_box.jpeg 480w, https://anoura12.github.io/blog/blog/p/write-like-agatha-christie-using-nanogpt/silly_hu941e12d02283ce2e345f7b36857c634a_77313_1024x0_resize_q75_box.jpeg 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Absolutely random&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;437&#34;
		data-flex-basis=&#34;1049px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;So, I tweaked the prompt so that the model generates text that continues the story. The prompt starts like this-&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Title: &amp;quot;The Enigmatic Garden Murder&amp;quot;

In the sleepy village of Willowbrook, nestled amidst rolling hills and lush greenery, 
a group of friends gather for their annual garden party. 
Lady Margaret Hastings, the elegant hostess known for her exquisite rose garden, 
has invited her closest friends and neighbors to revel in the beauty of her estate.        

As the afternoon sun bathes the garden in a warm, golden glow,
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And after waiting patiently for 30 mins as the model generated the sample, we get this&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Title: &amp;quot;The Enigmatic Garden Murder&amp;quot;

In the sleepy village of Willowbrook, nestled amidst rolling hills and lush greenery, 
a group of friends gather for their annual garden party. 
Lady Margaret Hastings, the elegant hostess known for her exquisite rose garden, 
has invited her closest friends and neighbors to revel in the beauty of her estate.        

As the afternoon sun bathes the garden in a warm, golden glow, 
a small, young man arrives in a beige chauffeur&#39;s car and picks up a young woman from the carousel. 
He smiles broadly and informs her that the afternoon&#39;s guests are all her relatives. 
They are not the sort of people she is used to seeing, however, and they demand to know where she is going. 
The young man reluctantly responds that she is going to the cottage of the Duke of Douglas.

The Duchess appears with her husband at once, but when the young man insists on taking his leave, 
she hesitates. Despite her chagrin, she gives him the keys to the cottage and asks him to escort her out.

Murdered: A Girl Who Loved the Water

The young man emerges from the cottage in a raincoat and comes out into the alleyway with his mistress. 
They approach each other.

&amp;quot;I should like to ask you something,&amp;quot; he says. &amp;quot;You&#39;re going to the cottage of the Duke of Douglas?&amp;quot;

&amp;quot;Yes, I am.&amp;quot;

&amp;quot;But what is the purpose for your visit?&amp;quot;

&amp;quot;I want to find out more about the painting in the house. It was a gift from a friend of mine. 
He did not appreciate the painting quite as I do.&amp;quot;

&amp;quot;He very nearly had it,&amp;quot; replied the maid. &amp;quot;He had an obstinate and irritable temper.&amp;quot;

&amp;quot;But how could he have done it?&amp;quot; asked the young man anxiously.

Dowager Margaret Hastings took a deep breath. &amp;quot;Well, he said it was too big and too much of a pain to 
take to get rid of it. He thought it too much like a picture that belonged to his parents. 
He said he would prefer to have it cleaned up. So he finally decided to destroy it.&amp;quot;

&amp;quot;Is that all?&amp;quot; inquired the maid, incredulously. &amp;quot;But how?&amp;quot;

&amp;quot;Oh, it&#39;s not so terrible,&amp;quot; replied Margaret Hastings with a smile. 
&amp;quot;It just took so long for him to kill the painting, and he was an old hand at that time, 
so he got in a little bit of trouble and had to meet with a certain fate.&amp;quot;

&amp;quot;You mean, he got caught up in it?&amp;quot; asked the maid. 
&amp;quot;Surely you didn&#39;t mean to kill him in the first place, did you? 
Why would you do such a thing?&amp;quot;

&amp;quot;Miss Bates, I was merely trying to amuse myself; you see, when the painting was at my disposal, 
I took it out and examined it closely, and I thought there was no danger to life or limb. 
But, as I said, the Duke of Douglas has a temper, and, as a result, he was very angry. 
Of course he pushed me aside and, when I tried to get away, he caught me up.&amp;quot;

&amp;quot;That&#39;s all right, my dear lady. You don&#39;t see any danger in it?&amp;quot;

&amp;quot;No, my dear Bates; it was simply a natural, even amusing thing that occurred. I didn&#39;t blame him for it. 
His temper was like mine. It&#39;s all right to be angry sometimes, and sometimes one is. 
In this case, it seems to me that he was quite justified. I had a pretty good instinct about such things; 
I knew the Duke of Douglas personally, and, of course, he was no friend of mine. He was a good man, 
a sensible man, and he knew he was not to blame for what had happened.&amp;quot;

Douglas personally, and, of course, he was no friend of mine. 
He was a good man, a sensible man, and he knew he was not to blame for what had happened.&amp;quot;

&amp;quot;And it was, indeed, your husband who had fallen into the trap?&amp;quot; asked the maid. 
&amp;quot;Did you say that he had fallen into the trap?&amp;quot;

&amp;quot;Not the trap myself, Miss Bates. But the Duke of Douglas has the right to dispose of the painting.&amp;quot;

&amp;quot;But why?&amp;quot; asked the maid in surprise. &amp;quot;Doesn&#39;t he have the right to dispose of the painting?&amp;quot;

&amp;quot;Yes; he did have the right to dispose of it,&amp;quot; said Margaret Hastings with a gentle smile. 
&amp;quot;There are rules governing this matter, Miss Bates; one must be content if one 
possesses no means of disposing of a work of art because it is his--and somebody else&#39;s--property. 
There are no grounds for anger in this matter.&amp;quot; The maid began to tremble. 
&amp;quot;Oh, my God!&amp;quot; she cried. &amp;quot;You&#39;re quite right! I have never erred in my life. 
I really believe the Duke of Douglas is guilty of murder!&amp;quot;

The maid&#39;s despair was quickly quelled. &amp;quot;I think,&amp;quot; said Margaret Hastings, 
&amp;quot;that your husband may have been justified. I really do. 
But that is the trouble: the Duke of Douglas has no right to dispose of the painting 
unless he has the right to dispose of it.&amp;quot;

&amp;quot;But what of, Miss Bates?&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This output is much much better and it actually does resemble her writing a lot even though it lacks overall structure to the story. For 10 iterations of finetuning, this isn&amp;rsquo;t bad at all. We can perhaps make this better by increasing the number of iterations in training or increasing batch_size but it might be a bit more computationally expensive to do so. Overall, it was a nice experiment and an interesting introduction to training and finetuning models.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Petals - A Game Changer in Training LLMs</title>
        <link>https://anoura12.github.io/blog/p/petals-a-game-changer-in-training-llms/</link>
        <pubDate>Mon, 25 Sep 2023 14:33:30 +0530</pubDate>
        
        <guid>https://anoura12.github.io/blog/p/petals-a-game-changer-in-training-llms/</guid>
        <description>&lt;img src="https://anoura12.github.io/blog/p/petals-a-game-changer-in-training-llms/petals.png" alt="Featured image of post Petals - A Game Changer in Training LLMs" /&gt;&lt;p&gt;Large Language Models or LLMs are all the hype nowadays with ChatGPT now being a part of our everyday lives. Naturally, the curious lot of us would like to understand how these LLMs work under the hood and run these models ourselves. To effectively run an LLM, we would need numerous GPU servers to handle the load of inference and model training making it computationally expensive to do so.&lt;/p&gt;
&lt;p&gt;But how expensive are we talking about here? For training models in particular, one of the fastest GPUs used is the A100 GPU. For reference, a single A100 GPU costs around $15,000. The Galactica model by Meta AI used 128 A100 GPUs for training. Along with high-end GPUs, it needs equally efficient CPUs. So, NVIDIA offers the DGX A100 which contains CPU compute power and 8 A100 GPUs which racks up to around $200,000. If we do the math, the total cost to fully train an LLM is a whopping $3.2 million. A little out of my price range if you ask me.&lt;/p&gt;
&lt;p&gt;The more feasible way to access LLM capabiities is through APIs like the OpenAI API or online platforms that have these models deployed and open for public use. However, a new open source tool called Petals opens the possibility of running these models in your own computer right in the comfort of your home.&lt;/p&gt;
&lt;h1 id=&#34;what-is-it-exactly&#34;&gt;What is it exactly?&lt;/h1&gt;
&lt;p&gt;Petals is an open source tool created by the &lt;a class=&#34;link&#34; href=&#34;https://techcrunch.com/tag/bigscience/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;BigScience&lt;/a&gt; community backed by startup, Hugging Face. This novel tool allows you to run an LLM in your own system through shared GPU servers hosted across the world on other machines. (These servers are contributed by people who offer their system&amp;rsquo;s GPU for use.) You download a small part of the model on your machine while the other servers run other parts of the model for inference and fine-tuning. Think Bit-Torrent for LLMs.&lt;/p&gt;
&lt;p&gt;This novel approach proves to be 10x faster than offloading (processes on a central server divided over multiple servers to increase speed and efficiency of the task) bringing about a more collaborative environment to train LLMs. Petals seeks to incentivise the process of contributing your system GPU where donators will receive “Bloom points” that they can spend on “higher priority or increased security guarantees” or potentially exchange for other rewards.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://anoura12.github.io/blog/blog/p/petals-a-game-changer-in-training-llms/petals_workflow.png&#34;
	width=&#34;1742&#34;
	height=&#34;680&#34;
	srcset=&#34;https://anoura12.github.io/blog/blog/p/petals-a-game-changer-in-training-llms/petals_workflow_hu0c85548ecaadd356da3ebf903d224a56_567826_480x0_resize_box_2.png 480w, https://anoura12.github.io/blog/blog/p/petals-a-game-changer-in-training-llms/petals_workflow_hu0c85548ecaadd356da3ebf903d224a56_567826_1024x0_resize_box_2.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;An overview of Petals&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;256&#34;
		data-flex-basis=&#34;614px&#34;
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;what-makes-it-so-novel&#34;&gt;What makes it so novel?&lt;/h1&gt;
&lt;p&gt;The popular method to running LLMs before was offloading. Here&amp;rsquo;s how lead Petals &lt;a class=&#34;link&#34; href=&#34;https://news.ycombinator.com/item?id=34216213&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;developer&lt;/a&gt;, Alexander Borzunov described offloading - &amp;ldquo;Imagine you have an A100 GPU with 80 GB memory and want to generate text with BLOOM, a 70-block transformer model with ~2.5 GB of weights per block. For each token, offloading will load the first 1/3 of the model (~27 blocks) from RAM/SSD to your GPU memory, run a forward pass through them, then free the memory and load the next 2/3, and so on.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;As opposed to APIs, offloading provided a lot of control to the internal weights of the LLM which is great for researchers however, due to the data transfer (we&amp;rsquo;re talking about transfering the model weights here, so around 175 GB transferred to a GPU in three intervals just to generate one token!) from RAM to GPUs, it leads to pretty high latency. This is particularly not favourable for applications like chatbots that need quick responses and rely on low-latency mechanisms.&lt;/p&gt;
&lt;p&gt;Petals is able to do inference 10x faster than offloading because it passes the neural network activation values (intermediate values or outputs produced at each layer of a neural network during the forward pass that represent the model&amp;rsquo;s internal representations) rather than all the weights. This results in a much lower latency value which is perfect for building LLM applications. (Something to also note is that Petals does have a lower throughput than offloading)&lt;/p&gt;
&lt;h1 id=&#34;how-does-it-work&#34;&gt;How does it work?&lt;/h1&gt;
&lt;p&gt;Petals works as a decentralised pipeline where a model is split into different blocks (transformer blocks) and hosted on different GPU servers. Each server holds consecutive blocks, the number of which depends on the server&amp;rsquo;s available GPU memory.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://anoura12.github.io/blog/blog/p/petals-a-game-changer-in-training-llms/petals_health.png&#34;
	width=&#34;1750&#34;
	height=&#34;572&#34;
	srcset=&#34;https://anoura12.github.io/blog/blog/p/petals-a-game-changer-in-training-llms/petals_health_hu5998f1de35730db3bd35b3129fa03d48_145872_480x0_resize_box_2.png 480w, https://anoura12.github.io/blog/blog/p/petals-a-game-changer-in-training-llms/petals_health_hu5998f1de35730db3bd35b3129fa03d48_145872_1024x0_resize_box_2.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;The health monitor of Petals showing the current status of the Stable Beluga 2 model&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;305&#34;
		data-flex-basis=&#34;734px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Output generation always takes place in an inference session. Before each inference session, the client is routed through a chain of servers that collectively holds models layers. Each server selects the most optimal set of blocks based on the current bottlenecks within the pipeline.&lt;/p&gt;
&lt;p&gt;In the inference session, the client stores a model&amp;rsquo;s embeddings locally. It then searches embedding vectors on the prefix token of the query from the local embeddings and sends these vectors to the servers. The servers compute these vectors and the final block sends the model representations (neural network activation values) back to the client which then computes probabilities of the next token based on these representations. While the session is active, servers store attention keys and values from past client inputs and use them for subsequent inference steps. To address potential failures, a client stores intermediate activations sent to each block and reroutes them from an offline server to an online node hosting the same block.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://anoura12.github.io/blog/blog/p/petals-a-game-changer-in-training-llms/petals_flow.png&#34;
	width=&#34;1920&#34;
	height=&#34;1080&#34;
	srcset=&#34;https://anoura12.github.io/blog/blog/p/petals-a-game-changer-in-training-llms/petals_flow_huc5ffefe640741a839a89b0101f788715_77454_480x0_resize_box_2.png 480w, https://anoura12.github.io/blog/blog/p/petals-a-game-changer-in-training-llms/petals_flow_huc5ffefe640741a839a89b0101f788715_77454_1024x0_resize_box_2.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;General workflow for one token&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;how-do-you-run-it-on-your-computer&#34;&gt;How do you run it on your computer?&lt;/h1&gt;
&lt;p&gt;It&amp;rsquo;s pretty simple to run Petals on your system. All you need to do is first install Petals&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install git+https://github.com/bigscience-workshop/petals
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;and then run the following code with your preferred open source LLM - this uses Stable Beluga 2 which a finetuned model of LLaMa 2.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from transformers import AutoTokenizer
from petals import AutoDistributedModelForCausalLM

# Choose any model available at https://health.petals.dev
model_name = &amp;quot;petals-team/StableBeluga2&amp;quot;

# Connect to a distributed network hosting model layers
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoDistributedModelForCausalLM.from_pretrained(model_name)

# Run the model as if it were on your computer
inputs = tokenizer(&amp;quot;Cat in french is&amp;quot;, return_tensors=&amp;quot;pt&amp;quot;)[&amp;quot;input_ids&amp;quot;]
outputs = model.generate(inputs, max_new_tokens=100)
print(tokenizer.decode(outputs[0])) 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;After running the code, you should get something like this. At the end, we can see the output, &amp;ldquo;Cat in french is chat.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://anoura12.github.io/blog/blog/p/petals-a-game-changer-in-training-llms/petals_terminal.png&#34;
	width=&#34;1572&#34;
	height=&#34;156&#34;
	srcset=&#34;https://anoura12.github.io/blog/blog/p/petals-a-game-changer-in-training-llms/petals_terminal_hu7da140fff99e5a29ade3be4a37639a78_43723_480x0_resize_box_2.png 480w, https://anoura12.github.io/blog/blog/p/petals-a-game-changer-in-training-llms/petals_terminal_hu7da140fff99e5a29ade3be4a37639a78_43723_1024x0_resize_box_2.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Code Output&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;1007&#34;
		data-flex-basis=&#34;2418px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;The line with &lt;code&gt;Route found: 0:40 via ...&lt;/code&gt; shows that blocks 0 to 40 belong to a GPU server with id ending with HfQWVM and blocks 40 to 80 belong to another GPU server with id ending with Zj98Se. We can see these servers on the Petals health monitor as well. The prompt, &amp;ldquo;Cat in french is&amp;rdquo; got converted into embedding vectors which ran through these blocks and gave the final outcome as &amp;ldquo;Cat in french is chat.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;One thing to note is that it does take some time to generate the output from around 15-20 secs to well over a minute depending on the number of the max tokens and the complexity of the prompt. I&amp;rsquo;d say the main advantage that Petals offers over ChatGPT is that it&amp;rsquo;s free and you get more control over the model&amp;rsquo;s internal states to which different adapters can be fitted to finetune the model.&lt;/p&gt;
&lt;p&gt;A pressing question that may come up is how secure even is this thing? In theory, the inputs of the model that someone else has given could be accessed through the intermediate states between blocks. Additionally, these servers belong to other people who potentially could have access to private data sent as input to the model for it be computed on these servers. To address these issue, Petals has recommended organisations to build private swarms containing GPUs by people that they trust so that their data isn&amp;rsquo;t compromised.&lt;/p&gt;
&lt;p&gt;In conclusion, I think Petals is great first step to decentralised compute for training LLMs. It opens a door of opportunities for those who can&amp;rsquo;t afford expensive GPUs to run these models which was earlier only restricted to rich corporations with the wherewithal to do so. I&amp;rsquo;m definitely excited to see what&amp;rsquo;s more to come.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Inception: Google Summer of Code - Week 1</title>
        <link>https://anoura12.github.io/blog/p/inception-google-summer-of-code-week-1/</link>
        <pubDate>Fri, 25 Jun 2021 18:46:43 +0530</pubDate>
        
        <guid>https://anoura12.github.io/blog/p/inception-google-summer-of-code-week-1/</guid>
        <description>&lt;img src="https://anoura12.github.io/blog/p/inception-google-summer-of-code-week-1/GSOC.jpg" alt="Featured image of post Inception: Google Summer of Code - Week 1" /&gt;&lt;h3 id=&#34;div-aligncenter-gsoc-2021-congratulations-your-proposal-with-incf-has-been-accepted-div&#34;&gt;&lt;!-- raw HTML omitted --&gt; &lt;em&gt;GSoC 2021: Congratulations, your proposal with INCF has been accepted!&lt;/em&gt; &lt;!-- raw HTML omitted --&gt;&lt;/h3&gt;
&lt;p&gt;I honestly couldn’t believe it. Till 11:17 PM that day, I was ridden with anxiety but it was all worth it when I finally saw the awaited email. Hours spent in proposal writing, maintaining constant communication with the project leads and handling ongoing uncertainties until now finally paid off.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://anoura12.github.io/blog/blog/p/inception-google-summer-of-code-week-1/gsoc.png&#34;
	width=&#34;1833&#34;
	height=&#34;691&#34;
	srcset=&#34;https://anoura12.github.io/blog/blog/p/inception-google-summer-of-code-week-1/gsoc_hu40fd995b94a4871007e1f0ffbea498ba_149691_480x0_resize_box_2.png 480w, https://anoura12.github.io/blog/blog/p/inception-google-summer-of-code-week-1/gsoc_hu40fd995b94a4871007e1f0ffbea498ba_149691_1024x0_resize_box_2.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;GSoC 2021&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;265&#34;
		data-flex-basis=&#34;636px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;The 2 month long project involves building a working prototype of AutSPACEs - a citizen science, participatory platform for, with and by the autistic community. It aims to capture their experiences with sensory processing difficulties. It sends this information to scientists who can generate better datasets and urge policy-makers to design and adapt spaces better suited to an autistic person’s needs. (Hence the name, AutSPACEs)&lt;/p&gt;
&lt;p&gt;The thing is, autism can’t be clearly categorised into separate levels. It’s different for everyone. That’s why making any advancements in this realm prove to be so difficult considering every individual has different needs and requirements. So, AutSPACEs reaches out to these autistic users to get information that is raw, authentic and true to each individual which scientists can rely on to get the best possible outcomes.&lt;/p&gt;
&lt;p&gt;AutSPACEs is developed using Python over a Django framework. It is connected to a backend database called Open Humans which is responsible for storing all the user experiences. Essentially, this database will be accessed through API calls where each user’s information will be stored as JSON data in a file. For the frontend, Bootstrap is being implemented to design the web pages with reference to simplified wireframes. The project has a lot of focus on the UI aspects as well because certain colours and layouts tend to trigger an autistic person’s senses. Hence, making the site as accessible as possible is a priority.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://anoura12.github.io/blog/blog/p/inception-google-summer-of-code-week-1/Citscicartoon.png&#34;
	width=&#34;1062&#34;
	height=&#34;849&#34;
	srcset=&#34;https://anoura12.github.io/blog/blog/p/inception-google-summer-of-code-week-1/Citscicartoon_hu44da07b71cf8c59c694935722b47d95b_411231_480x0_resize_box_2.png 480w, https://anoura12.github.io/blog/blog/p/inception-google-summer-of-code-week-1/Citscicartoon_hu44da07b71cf8c59c694935722b47d95b_411231_1024x0_resize_box_2.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;How AutSPACEs Works&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;125&#34;
		data-flex-basis=&#34;300px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;The project is currently in its early stages with the immediate goal of creating an MVP or a minimum viable product of the site. I’m really looking forward to helping the team achieve this!&lt;/p&gt;
&lt;p&gt;My Google Summer of Code journey took off with the community bonding period. Right after my first interaction with my mentors, Kirstie Whitaker and Lotty Coupat, and the autistic community along with project lead, Georgia Aitkenhead and head developer, James Kim, I was absolutely delighted to be a part of this amazing team. The meetups and the community calls provide a really lively atmosphere to share ideas and collaborate with the moderators and the dev team to reach viable solutions.&lt;/p&gt;
&lt;p&gt;I can’t wait to see what’s in store for me in the next two months. To infinity and beyond!&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
